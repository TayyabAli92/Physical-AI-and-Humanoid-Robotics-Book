"use strict";(globalThis.webpackChunkbook_project=globalThis.webpackChunkbook_project||[]).push([[2101],{6404:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/intro","title":"Module 4 \u2013 Vision-Language-Action","description":"This module explores Vision-Language-Action (VLA) models, integrating visual perception and natural language understanding for complex robotic task planning and execution.","source":"@site/docs/module-4-vla/intro.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/intro","permalink":"/physical-ai-robotics-book/docs/module-4-vla/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/TayyabAli92/Physical-AI-and-Humanoid-Robotics-Book/edit/main/book-project/docs/module-4-vla/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialsSidebar","previous":{"title":"Navigation (Nav2): Path Planning and Autonomous Movement","permalink":"/physical-ai-robotics-book/docs/module-3-isaac/chapter-3-navigation-nav2"},"next":{"title":"Whisper: Speech Recognition for Robot Interaction","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-1-whisper"}}');var t=o(4848),a=o(8453);const r={sidebar_position:1},l="Module 4 \u2013 Vision-Language-Action",s={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Chapter List",id:"chapter-list",level:2},{value:"Overview",id:"overview",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4--vision-language-action",children:"Module 4 \u2013 Vision-Language-Action"})}),"\n",(0,t.jsx)(n.p,{children:"This module explores Vision-Language-Action (VLA) models, integrating visual perception and natural language understanding for complex robotic task planning and execution."}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement Vision-Language-Action models for robotic applications"}),"\n",(0,t.jsx)(n.li,{children:"Integrate multimodal perception for humanoid robot control"}),"\n",(0,t.jsx)(n.li,{children:"Apply natural language processing for robot task planning"}),"\n",(0,t.jsx)(n.li,{children:"Combine perception, reasoning, and action for intelligent behavior"}),"\n",(0,t.jsx)(n.li,{children:"Develop end-to-end systems for autonomous humanoid capabilities"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"chapter-list",children:"Chapter List"}),"\n",(0,t.jsx)(n.p,{children:"This module contains the following chapters:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Chapter 1: Whisper - Speech recognition and processing for robot interaction"}),"\n",(0,t.jsx)(n.li,{children:"Chapter 2: LLM Planning - Large language models for robotic task planning"}),"\n",(0,t.jsx)(n.li,{children:"Chapter 3: Capstone - Autonomous humanoid project integrating all modules"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action models represent the cutting edge of robotic intelligence, enabling robots to perceive their environment, understand natural language commands, and execute complex actions. This module integrates all previous knowledge into a comprehensive humanoid robotics system."}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TD\n    A[Vision-Language-Action] --\x3e B[Visual Perception]\n    A --\x3e C[Natural Language]\n    A --\x3e D[Robotic Action]\n    B --\x3e E[Integrated Control]\n    C --\x3e E\n    D --\x3e E\n    E --\x3e F[Humanoid Intelligence]"})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>l});var i=o(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);