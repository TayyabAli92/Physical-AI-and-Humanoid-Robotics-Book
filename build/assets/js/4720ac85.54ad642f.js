"use strict";(globalThis.webpackChunkbook_project=globalThis.webpackChunkbook_project||[]).push([[4955],{5975:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/chapter-1-whisper","title":"Whisper: Speech Recognition for Robot Interaction","description":"Learn to integrate OpenAI Whisper for robust speech-to-text capabilities, enabling robots to understand spoken commands and environmental audio cues.","source":"@site/docs/module-4-vla/chapter-1-whisper.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-1-whisper","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-1-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/TayyabAli92/Physical-AI-and-Humanoid-Robotics-Book/edit/main/book-project/docs/module-4-vla/chapter-1-whisper.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialsSidebar","previous":{"title":"Module 4 \u2013 Vision-Language-Action","permalink":"/physical-ai-robotics-book/docs/module-4-vla/intro"},"next":{"title":"LLM Planning: Large Language Models for Robotic Task Planning","permalink":"/physical-ai-robotics-book/docs/module-4-vla/chapter-2-llm-planning"}}');var i=o(4848),r=o(8453);const a={},s="Whisper: Speech Recognition for Robot Interaction",c={},l=[];function d(e){const n={h1:"h1",header:"header",li:"li",mermaid:"mermaid",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"whisper-speech-recognition-for-robot-interaction",children:"Whisper: Speech Recognition for Robot Interaction"})}),"\n",(0,i.jsx)(n.p,{children:"Learn to integrate OpenAI Whisper for robust speech-to-text capabilities, enabling robots to understand spoken commands and environmental audio cues."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understanding Whisper architecture and capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Integrating Whisper with robotic systems"}),"\n",(0,i.jsx)(n.li,{children:"Processing real-time audio streams"}),"\n",(0,i.jsx)(n.li,{children:"Handling noise and environmental challenges"}),"\n",(0,i.jsx)(n.li,{children:"Converting speech to actionable commands"}),"\n"]}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TD\n    A[Audio Input] --\x3e B[Whisper Model]\n    B --\x3e C[Speech-to-Text]\n    C --\x3e D[Command Parsing]\n    D --\x3e E[Robot Action]\n    A --\x3e F[Noise Filtering]\n    F --\x3e B"})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>s});var t=o(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);